{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part 2 - Getting Started with PyTorch ",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chetansolanke14/NaturalLanguageProcessing/blob/master/PyCon/Part_2_Getting_Started_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtypjxXsGpuY",
        "colab_type": "text"
      },
      "source": [
        "## Part 2 - Getting Started with PyTorch\n",
        "\n",
        "**Required Time:  20 minutes**\n",
        "\n",
        "This notebook builds on top of the first block, covering basic concepts useful to understand the PyTorch deep learning framework such as objective function, non-linearities, affine maps, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### Journey\n",
        "- Computation Graph\n",
        "- **Exercise: **Computing the derivative\n",
        "- Input\n",
        "- **Exercise:** Tensor transformation\n",
        "- Linear Tranformation\n",
        "- Non-Linear Transformation\n",
        "- **Exercise:** Chaining Linear Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnDSo-MOISyp",
        "colab_type": "text"
      },
      "source": [
        "### Computation Graph - An Overview\n",
        "A simplified definition of a neural network is a string of functions that are differentiable and that we can combine together to get more complicated functions. An intuitive way to express this process is through computation graphs. PyTorch provide efficient functionalities for **automatic differentiation**.\n",
        "\n",
        "![alt txt](https://camo.githubusercontent.com/c665d8f2e5bf67bc93573cc3a7fb7d26028596ca/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d4261636b70726f702f696d672f747265652d6576616c2d6465726976732e706e67)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9xz811L6sC5",
        "colab_type": "code",
        "outputId": "4f37cb95-d136-43f6-92cc-b7a8020ce810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "### FORWARD\n",
        "print(\"---------------------------------------------------\")\n",
        "print(\"FORWARD: \")\n",
        "\n",
        "# layer 1\n",
        "a = torch.tensor(2.0, requires_grad=True)\n",
        "b = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# layer 2\n",
        "c = a + b\n",
        "c.retain_grad() # retrain gradients for non-leaf Tensors\n",
        "\n",
        "d = b + 1.0\n",
        "d.retain_grad() # retrain gradients for non-leaf Tensors\n",
        "\n",
        "# layer 3\n",
        "e = c * d\n",
        "\n",
        "print(\"e: \", e)\n",
        "\n",
        "### BACKWARD\n",
        "print(\"---------------------------------------------------\")\n",
        "print(\"BACKWARD: \")\n",
        "\n",
        "e.backward()\n",
        "\n",
        "print(\"c.grad: \", c.grad.detach().item()) # de/dc\n",
        "print(\"d.grad: \", d.grad.detach().item()) # de/dd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------------------\n",
            "FORWARD: \n",
            "e:  tensor(6., grad_fn=<MulBackward0>)\n",
            "---------------------------------------------------\n",
            "BACKWARD: \n",
            "c.grad:  2.0\n",
            "d.grad:  3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcLT3vY5OdRj",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### EXERCISE - Computing the derivative\n",
        "\n",
        "Compute the derivates of `a` and `b` with respect to c. Just the left part of the figure above.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmwqps3-OXkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7IN-geRSAE6",
        "colab_type": "text"
      },
      "source": [
        "Here is a nice [tutorial](https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec) discussing in detail how gradients are handled and computed in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtcM1NLcc5_A",
        "colab_type": "text"
      },
      "source": [
        "### Input\n",
        "The first component of the neural network is the input. Inputs have to be represented in tensor formats as this is the main data structure or representation used in PyTorch. We were introduced to tensors and a few operations that are possible with them in the previous segment of this tutorial. Therefore, we will briefly review different kinds of inputs that are common in NLP. This is the first actual part of the tutorial where we start to introduce concepts related to NLP and how we will integrate them with the other components provided in the PyTorch ecosystem. Inputs can be represented in either scalars, vectors, or multi-dimensional matrices. Whichever the type, they are all represented as tensors in PyTorch. Typically, the inputs are composed from publicly available datasets.\n",
        "\n",
        "Inputs to an NLP deep learning model are usually of the following dimensions: `batch_size X max_sequence_lenght X vocab_size`. Let's assume our batch size is 6, the sequence length is 60, and vocab size is 10000. Let's see how this looks below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoXYKBvudIMX",
        "colab_type": "code",
        "outputId": "cedd3a44-cb87-4389-c68a-c897a59b942f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample = torch.rand(64, 60, 10000)\n",
        "print(sample.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 60, 10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRk_rJsPglp6",
        "colab_type": "text"
      },
      "source": [
        "The first thing you will notice is the huge size in vocbulary, which is typical when using what's called one-hot encodings. There is also option to encode words and sentences into efficient embeddings. This ensure a more efficient representation as words can be represented to have semantic relationship.\n",
        "\n",
        "In such case, the dimensions reduced and they typically look like the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be0ItrG_gcF_",
        "colab_type": "code",
        "outputId": "5939a3b7-d1b0-40bb-cf57-81dbdcd4fff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_with_embeddings = torch.rand(64, 60, 100)\n",
        "print(sample.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 60, 10000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fss0_5EDgmU8",
        "colab_type": "text"
      },
      "source": [
        "Note now that the 3rd dimension has been significantly reduced in dimensoin because we are using embeddings as input to represente sequences. This not only ensure efficiency in terms of meaning buy also the network will train more efficiently because the dimension are reduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcPR_vNZe68J",
        "colab_type": "text"
      },
      "source": [
        "----\n",
        "### EXERCISE - Tensor Transformation\n",
        "\n",
        "Sometimes we need to permute the dimensions of the tensor. How dow we do this in PyTorch? Please visit the PyTorch documentation to find out how to achieve a tranformation of the original size of the tensor. Hint: `A.permute()`. Try to permute the `sample_with_embeddings` above to be of the following dimenions instead: `max_sequence_lenght X batch_size X vocab_size`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR_3YmWUfVY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE GOES HERE\n",
        "\n",
        "\n",
        "### YOUR CODE GOES HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6n95TEFIyxd",
        "colab_type": "text"
      },
      "source": [
        "### Linear Transformation\n",
        "A fundamental operation of training a neural network is affine mapping or linear transformations, which is simply a tranformation of a tensor given some function. PyTorch already packages various linear transformations, so we don't need to manually implement them. \n",
        "\n",
        "Let's look at the example below. We wish to output the hidden representation using randomly initialized weight and biases. In other words, we wish to compute the following:\n",
        "\n",
        "\n",
        "$$\n",
        "y = Wx + b\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG1m-KvnnP6Y",
        "colab_type": "code",
        "outputId": "85f63fdf-29c9-400b-ecbf-00f933412978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# using Linear unit in PyTorch\n",
        "\n",
        "sample_x = torch.rand(64, 60, 100)\n",
        "\n",
        "fc = nn.Linear(100, 50) # Wx + b\n",
        "\n",
        "# chaining happening automatically\n",
        "out = fc(sample_x)\n",
        "print(out.size())\n",
        "\n",
        "print(sample_x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 60, 50])\n",
            "tensor([[[0.2218, 0.8871, 0.7491,  ..., 0.2637, 0.0226, 0.5044],\n",
            "         [0.4265, 0.7755, 0.3018,  ..., 0.0461, 0.7320, 0.3729],\n",
            "         [0.8693, 0.9141, 0.4076,  ..., 0.2277, 0.2975, 0.9917],\n",
            "         ...,\n",
            "         [0.4049, 0.9269, 0.6229,  ..., 0.5807, 0.5033, 0.3958],\n",
            "         [0.1866, 0.5652, 0.2095,  ..., 0.2580, 0.9312, 0.8549],\n",
            "         [0.1859, 0.9515, 0.8794,  ..., 0.4411, 0.2722, 0.1611]],\n",
            "\n",
            "        [[0.9518, 0.6685, 0.6872,  ..., 0.3222, 0.5740, 0.3342],\n",
            "         [0.3053, 0.3433, 0.8695,  ..., 0.0735, 0.6718, 0.6849],\n",
            "         [0.2689, 0.7600, 0.0173,  ..., 0.2909, 0.6432, 0.5380],\n",
            "         ...,\n",
            "         [0.1556, 0.8891, 0.0601,  ..., 0.2868, 0.2580, 0.2419],\n",
            "         [0.8643, 0.3586, 0.9642,  ..., 0.9187, 0.3414, 0.5323],\n",
            "         [0.6241, 0.1440, 0.1963,  ..., 0.1102, 0.8434, 0.0396]],\n",
            "\n",
            "        [[0.2949, 0.2038, 0.7452,  ..., 0.0172, 0.2478, 0.5980],\n",
            "         [0.8738, 0.4244, 0.5385,  ..., 0.9600, 0.8836, 0.9857],\n",
            "         [0.9358, 0.0936, 0.6502,  ..., 0.8722, 0.0607, 0.2987],\n",
            "         ...,\n",
            "         [0.0568, 0.0425, 0.4496,  ..., 0.8270, 0.6357, 0.6995],\n",
            "         [0.1203, 0.5434, 0.1182,  ..., 0.0455, 0.0751, 0.7110],\n",
            "         [0.8255, 0.9155, 0.4824,  ..., 0.3949, 0.6483, 0.6332]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.5530, 0.0696, 0.6027,  ..., 0.5961, 0.0665, 0.1013],\n",
            "         [0.4199, 0.9753, 0.6202,  ..., 0.6888, 0.3617, 0.2175],\n",
            "         [0.4776, 0.1852, 0.1537,  ..., 0.1085, 0.1652, 0.3395],\n",
            "         ...,\n",
            "         [0.1322, 0.9896, 0.1758,  ..., 0.2566, 0.9062, 0.8268],\n",
            "         [0.8387, 0.5719, 0.7428,  ..., 0.5019, 0.1060, 0.3479],\n",
            "         [0.4779, 0.1389, 0.5965,  ..., 0.8736, 0.6292, 0.3351]],\n",
            "\n",
            "        [[0.1437, 0.0853, 0.1144,  ..., 0.1501, 0.3101, 0.0713],\n",
            "         [0.1619, 0.9433, 0.9215,  ..., 0.9931, 0.9661, 0.0263],\n",
            "         [0.9138, 0.0738, 0.9779,  ..., 0.6248, 0.5842, 0.6191],\n",
            "         ...,\n",
            "         [0.9008, 0.4449, 0.8869,  ..., 0.7023, 0.1128, 0.3331],\n",
            "         [0.8131, 0.0959, 0.2409,  ..., 0.5758, 0.7010, 0.0444],\n",
            "         [0.6769, 0.8274, 0.4405,  ..., 0.2008, 0.0481, 0.1943]],\n",
            "\n",
            "        [[0.9958, 0.7816, 0.7066,  ..., 0.8445, 0.0429, 0.9236],\n",
            "         [0.0810, 0.9149, 0.7927,  ..., 0.6909, 0.8914, 0.4826],\n",
            "         [0.5011, 0.9676, 0.2471,  ..., 0.8141, 0.4707, 0.3989],\n",
            "         ...,\n",
            "         [0.7858, 0.8291, 0.0656,  ..., 0.2412, 0.5435, 0.4383],\n",
            "         [0.1357, 0.4425, 0.6283,  ..., 0.2356, 0.7965, 0.4353],\n",
            "         [0.5542, 0.9859, 0.5453,  ..., 0.2000, 0.7083, 0.3261]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFFk5EMRRO2J",
        "colab_type": "text"
      },
      "source": [
        "### Non-linear Transformation\n",
        "\n",
        "We can then apply a non-linear transformation using the results of the previous linear transformation, computed as follows:\n",
        "\n",
        "$$\n",
        "h = sigma(Wx + b)\n",
        "$$\n",
        "\n",
        "where sigma refers to the `Sigmoid` activation function in our example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPj24LjjSBQI",
        "colab_type": "code",
        "outputId": "7557ce2e-1d30-4336-a657-8b78f71b9d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_x = torch.rand(64, 60, 100)\n",
        "\n",
        "fc = nn.Linear(100, 50)\n",
        "sig = nn.Sigmoid()\n",
        "\n",
        "out = fc(sample_x)\n",
        "out = sig(out) # [-1, 1]\n",
        "\n",
        "print(out.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 60, 50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03YCht70d99L",
        "colab_type": "text"
      },
      "source": [
        "There are other popular non-linear transformation or activiation functions available for use such as `RelU` and `tanh`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtSpoYjPuxTz",
        "colab_type": "text"
      },
      "source": [
        "### Softmax Classifier\n",
        "This component of the neural network is called the classifier, which is usually in charge of making the final prediction via a normalized representation of the output layer. From the equation below you can see that to get this output we just need to apply a softmax function. The values returned will be in the range (0, 1) and sum to 1.\n",
        "\n",
        "$$\n",
        "output = softmax (x)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkE2xd88u74F",
        "colab_type": "code",
        "outputId": "54a82234-ed28-4a68-f394-40f0ffa0fd21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "m = nn.Softmax(dim=1)\n",
        "x = torch.randn(4, 5)\n",
        "out = m(x)\n",
        "print(out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0635, 0.0867, 0.1149, 0.5673, 0.1677],\n",
            "        [0.0936, 0.0555, 0.5116, 0.1520, 0.1873],\n",
            "        [0.1764, 0.3617, 0.0753, 0.1635, 0.2230],\n",
            "        [0.2519, 0.1645, 0.2851, 0.0536, 0.2448]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiCzQkrIo3Kf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "### EXERCISE - Chaining Linear Layers\n",
        "\n",
        "Go ahead and try to chain a few linear transformations, make it deep if you like. Revise the previous notebook to help you build a chain of operations. \n",
        "\n",
        "Feel free to explore the PyTorch documentation to familiarize yourself with more of the basic linear and non-linear transformations. In addition, try to change the size of the `Linear` layers and combining a series of them.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qGEs1mzgAlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOh_PMYY8gwr",
        "colab_type": "text"
      },
      "source": [
        "##  Beyond\n",
        "Before proceeding to the next section, it would be of great help to review the following notebooks. For the purpose of this tutorial, you can skip this part.\n",
        "\n",
        "- [NN from scratch with PyTorch](https://colab.research.google.com/drive/109gHWFUlUzuwhgXROpzIuVoSPZA_qeoy) - This notebook shows you how to implement a neural network from scratch using PyTorch\n",
        "- [RNN from scratch](https://colab.research.google.com/drive/1NVuWLZ0cuXPAtwV4Fs2KZ2MNla0dBUas) - This notebook shows you how to ment recurrent neural networks (RNNs) using PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmPKBtYohRWJ",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ew57XbHr5Q",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "- [NN from scratch with PyTorch](https://colab.research.google.com/drive/109gHWFUlUzuwhgXROpzIuVoSPZA_qeoy)\n",
        "- [RNN from scratch](https://colab.research.google.com/drive/1NVuWLZ0cuXPAtwV4Fs2KZ2MNla0dBUas)\n",
        "- [Emotion Recognition with PyTorch](https://github.com/omarsar/appworks_meetup_2018/blob/master/Deep%20Learning%20Emotion%20Recognition%20PyTorch.ipynb)"
      ]
    }
  ]
}